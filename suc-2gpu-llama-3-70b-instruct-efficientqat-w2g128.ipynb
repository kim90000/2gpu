{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30919,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\n\noutput_dir = '../output/'\nfile_to_delete = os.path.join(output_dir, ' اسم_الملف_الذي_تريد_حذفه ') # استبدل ' اسم_الملف_الذي_تريد_حذفه ' بالاسم الفعلي للملف\n\nif os.path.exists(file_to_delete):\n    os.remove(file_to_delete)\n    print(f\"تم حذف الملف: {file_to_delete}\")\nelse:\n    print(f\"الملف غير موجود: {file_to_delete}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install optimum\n!pip install auto-gptq\n!pip install gptqmodel\n!pip uninstall scipy -y\n!pip install scipy\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install optimum\n!pip install auto-gptq","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip uninstall -y numpy scipy\n!pip install numpy==1.26.0 scipy==1.11.3","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-09T03:28:29.206679Z","iopub.execute_input":"2025-03-09T03:28:29.206984Z","iopub.status.idle":"2025-03-09T03:28:39.870154Z","shell.execute_reply.started":"2025-03-09T03:28:29.206961Z","shell.execute_reply":"2025-03-09T03:28:39.869083Z"}},"outputs":[{"name":"stdout","text":"Found existing installation: numpy 1.26.0\nUninstalling numpy-1.26.0:\n  Successfully uninstalled numpy-1.26.0\nFound existing installation: scipy 1.11.3\nUninstalling scipy-1.11.3:\n  Successfully uninstalled scipy-1.11.3\nCollecting numpy==1.26.0\n  Using cached numpy-1.26.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (58 kB)\nCollecting scipy==1.11.3\n  Using cached scipy-1.11.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (60 kB)\nUsing cached numpy-1.26.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.2 MB)\nUsing cached scipy-1.11.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (36.4 MB)\nInstalling collected packages: numpy, scipy\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ngptqmodel 2.0.0 requires numpy>=2.2.2, but you have numpy 1.26.0 which is incompatible.\nlangchain 0.3.12 requires async-timeout<5.0.0,>=4.0.0; python_version < \"3.11\", but you have async-timeout 5.0.1 which is incompatible.\nmkl-fft 1.3.8 requires numpy<1.27.0,>=1.26.4, but you have numpy 1.26.0 which is incompatible.\nmkl-random 1.2.4 requires numpy<1.27.0,>=1.26.4, but you have numpy 1.26.0 which is incompatible.\nmkl-umath 0.1.1 requires numpy<1.27.0,>=1.26.4, but you have numpy 1.26.0 which is incompatible.\nmlxtend 0.23.3 requires scikit-learn>=1.3.1, but you have scikit-learn 1.2.2 which is incompatible.\npandas-gbq 0.25.0 requires google-api-core<3.0.0dev,>=2.10.2, but you have google-api-core 1.34.1 which is incompatible.\nplotnine 0.14.4 requires matplotlib>=3.8.0, but you have matplotlib 3.7.5 which is incompatible.\ntensorflow 2.17.1 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3, but you have protobuf 6.30.0 which is incompatible.\ntensorflow-decision-forests 1.10.0 requires tensorflow==2.17.0, but you have tensorflow 2.17.1 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed numpy-1.26.0 scipy-1.11.3\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from transformers import AutoTokenizer, LlamaForCausalLM\nimport torch\n\nquant_dir = \"ChenMnZ/Llama-3-70b-instruct-EfficientQAT-w2g128-GPTQ\"\ntokenizer = AutoTokenizer.from_pretrained(quant_dir, use_fast=True)\n\n# استخدام device_map=\"balanced\" لتوزيع متوازن\nmodel = LlamaForCausalLM.from_pretrained(\n    quant_dir,\n    device_map=\"auto\",  # يجب أن يوزع بالتساوي بين الأجهزة المتاحة\n    load_in_8bit=False,     # لا تستخدم تكميم 8-bit\n    load_in_4bit=False      # لا تستخدم تكميم 4-bit\n)\n\n# طباعة معلومات عن استخدام الذاكرة\nfor i in range(torch.cuda.device_count()):\n    used_mem = torch.cuda.memory_allocated(i) / 1024**3\n    print(f\"GPU {i}: {used_mem:.2f} GB مستخدمة\")\n\n# تشغيل الاستدلال\ninput_text = \"Model quantization is\"\ninputs = tokenizer(input_text, return_tensors=\"pt\")\nwith torch.no_grad():\n    output = model.generate(**inputs, max_length=50)\nprint(tokenizer.decode(output[0]))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-09T03:28:50.836460Z","iopub.execute_input":"2025-03-09T03:28:50.836811Z","iopub.status.idle":"2025-03-09T03:39:07.410249Z","shell.execute_reply.started":"2025-03-09T03:28:50.836782Z","shell.execute_reply":"2025-03-09T03:39:07.409094Z"}},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"name":"stdout","text":"\n\u001b[32mINFO\u001b[0m  ENV: Auto setting PYTORCH_CUDA_ALLOC_CONF='expandable_segments:True' for memory saving.      \n\u001b[32mINFO\u001b[0m  ENV: Auto setting CUDA_DEVICE_ORDER=PCI_BUS_ID for correctness.                              \n","output_type":"stream"},{"name":"stderr","text":"Detected gptqmodel and auto-gptq, will use gptqmodel\n","output_type":"stream"},{"name":"stdout","text":"\u001b[32mINFO\u001b[0m  Kernel: Auto-selection: adding candidate `TorchQuantLinear`                                  \n","output_type":"stream"},{"name":"stderr","text":"`loss_type=None` was set in the config but it is unrecognised.Using the default loss: `ForCausalLMLoss`.\nDetected gptqmodel and auto-gptq, will use gptqmodel\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9c3fa9ac2cb347049bfb2a600ea072a0"}},"metadata":{}},{"name":"stdout","text":"\u001b[32mINFO\u001b[0m  Optimize: `TorchQuantLinear` compilation triggered.                                          \n","output_type":"stream"},{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"GPU 0: 8.79 GB مستخدمة\nGPU 1: 12.21 GB مستخدمة\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:2105: UserWarning: You are calling .generate() with the `input_ids` being on a device type different than your model's device. `input_ids` is on cpu, whereas the model is on cuda. You may experience unexpected behaviors or slower generation. Please make sure that you have put `input_ids` to the correct device by calling for example input_ids = input_ids.to('cuda') before running `.generate()`.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"Model quantization is a good starting point for understanding the concept of a \"good\" or \"bad\" person. It is important to recognize that people are complex and multifaceted, and that their actions and beliefs can be influenced by a wide range\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"with torch.no_grad():\n    output = model.generate(**inputs, max_length=50, do_sample=True)\nprint(tokenizer.decode(output[0]))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from transformers import AutoTokenizer, LlamaForCausalLM\nimport torch\n\nquant_dir = \"ChenMnZ/Llama-3-70b-instruct-EfficientQAT-w2g128-GPTQ\"\ntokenizer = AutoTokenizer.from_pretrained(quant_dir, use_fast=True)\n\n# استخدام device_map=\"balanced\" لتوزيع متوازن\nmodel = LlamaForCausalLM.from_pretrained(\n    quant_dir,\n    device_map=\"auto\",  # يجب أن يوزع بالتساوي بين الأجهزة المتاحة\n    load_in_8bit=False,     # لا تستخدم تكميم 8-bit\n    load_in_4bit=False      # لا تستخدم تكميم 4-bit\n)\n\n# طباعة معلومات عن استخدام الذاكرة\nfor i in range(torch.cuda.device_count()):\n    used_mem = torch.cuda.memory_allocated(i) / 1024**3\n    print(f\"GPU {i}: {used_mem:.2f} GB مستخدمة\")\n\n# تشغيل الاستدلال\ninput_text = \"Model quantization is\"\ninputs = tokenizer(input_text, return_tensors=\"pt\")\nwith torch.no_grad():\n    output = model.generate(**inputs, max_length=50)\nprint(tokenizer.decode(output[0]))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"kaitchup/Llama-3.3-70B-Instruct-AutoRoundGPTQ-2bit","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}