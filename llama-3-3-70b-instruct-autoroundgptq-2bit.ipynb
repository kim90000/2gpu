{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30919,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install optimum\n!pip install auto-gptq\n!pip install gptqmodel\n!pip uninstall scipy -y\n!pip install scipy\n!pip uninstall -y numpy scipy\n!pip install numpy==1.26.0 scipy==1.11.3","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install auto-round[gpu]","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from transformers import AutoTokenizer, LlamaForCausalLM\nimport torch\n\nquant_dir = \"kaitchup/Llama-3.3-70B-Instruct-AutoRoundGPTQ-2bit\"\ntokenizer = AutoTokenizer.from_pretrained(quant_dir, use_fast=True)\n\n# استخدام device_map=\"balanced\" لتوزيع متوازن\nmodel = LlamaForCausalLM.from_pretrained(\n    quant_dir,\n    device_map=\"auto\",  # يجب أن يوزع بالتساوي بين الأجهزة المتاحة\n    load_in_8bit=False,     # لا تستخدم تكميم 8-bit\n    load_in_4bit=False      # لا تستخدم تكميم 4-bit\n)\n\n# طباعة معلومات عن استخدام الذاكرة\nfor i in range(torch.cuda.device_count()):\n    used_mem = torch.cuda.memory_allocated(i) / 1024**3\n    print(f\"GPU {i}: {used_mem:.2f} GB مستخدمة\")\n\n# تشغيل الاستدلال\ninput_text = \"Model quantization is\"\ninputs = tokenizer(input_text, return_tensors=\"pt\")\nwith torch.no_grad():\n    output = model.generate(**inputs, max_length=50)\nprint(tokenizer.decode(output[0]))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from transformers import AutoModelForCausalLM, AutoTokenizer\nfrom auto_round import AutoRoundConfig\n\nbackend = \"auto\"  ##cpu, hpu, cuda\nquantization_config = AutoRoundConfig(\n    backend=backend\n)\nquantized_model_path = \"kaitchup/Llama-3.3-70B-Instruct-AutoRoundGPTQ-2bit\"\nmodel = AutoModelForCausalLM.from_pretrained(quantized_model_path,\n                                             device_map=backend.split(':')[0],\n                                             quantization_config=quantization_config)\ntokenizer = AutoTokenizer.from_pretrained(quantized_model_path)\ntext = \"There is a girl who likes adventure,\"\ninputs = tokenizer(text, return_tensors=\"pt\").to(model.device)\nprint(tokenizer.decode(model.generate(**inputs, max_new_tokens=5)[0]))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-09T04:17:55.327003Z","iopub.execute_input":"2025-03-09T04:17:55.327418Z"}},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/auto_round/auto_quantizer.py:190: UserWarning: You passed `quantization_config` or equivalent parameters to `from_pretrained` but the model you're loading already has a `quantization_config` attribute. The `quantization_config` from the model will be used.However, loading attributes (e.g. ['target_backend']) will be overwritten with the one you passed to `from_pretrained`. The rest will be ignored.\n  warnings.warn(warning_msg)\n","output_type":"stream"},{"name":"stdout","text":"\n\u001b[32mINFO\u001b[0m  ENV: Auto setting PYTORCH_CUDA_ALLOC_CONF='expandable_segments:True' for memory saving.      \n\u001b[32mINFO\u001b[0m  ENV: Auto setting CUDA_DEVICE_ORDER=PCI_BUS_ID for correctness.                              \n","output_type":"stream"},{"name":"stderr","text":"Detected gptqmodel and auto-gptq, will use gptqmodel\n","output_type":"stream"},{"name":"stdout","text":"\u001b[32mINFO\u001b[0m  Kernel: Auto-selection: adding candidate `TorchQuantLinear`                                  \n","output_type":"stream"},{"name":"stderr","text":"`loss_type=None` was set in the config but it is unrecognised.Using the default loss: `ForCausalLMLoss`.\nDetected gptqmodel and auto-gptq, will use gptqmodel\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"373a1b88f8334ac4a006dfdc1027a1ed"}},"metadata":{}}],"execution_count":null}]}